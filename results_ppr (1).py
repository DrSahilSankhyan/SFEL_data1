# -*- coding: utf-8 -*-
"""Results - ppr

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mxT2LGYmXluPoc107S9w39WZc_MBPxRf
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.metrics import r2_score, mean_absolute_error
import matplotlib.pyplot as plt

data=pd.read_csv("ML_Data_18.csv")

data.head()

data=data.drop(columns=["Longitude","Latitude"])

data.head()

from sklearn.preprocessing import MinMaxScaler

# Create a MinMaxScaler object
scaler = MinMaxScaler(feature_range=(-1,1))

# Normalize the data and update the DataFrame in place
data[:] = scaler.fit_transform(data)

data.head()

corr_matrix= data.corr()

print(data.shape)

plt.figure(figsize=(27, 25))  # Adjusted for better readability
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', cbar=True)
plt.title('Correlation Heatmap with Annotations')
plt.show()

all_columns = list(data.columns)
print(all_columns)

porosity_index = all_columns.index("Porosity")
# Split into X and y
X = data.iloc[:, :porosity_index + 1]      # All columns up to and including "Porosity"
y = data.iloc[:, porosity_index + 1:]      # All columns after "Porosity" till "5-25"

print("X shape:", X.shape)
print("y shape:", y.shape)
print("X columns:", X.columns.tolist())
print("y columns:", y.columns.tolist())

corr_matrix=X.corr().abs()

threshold=0.78
to_drop= set()

for i in range(len(corr_matrix.columns)):
    for j in range(i+1,len(corr_matrix.columns)):
        if corr_matrix.iloc[i,j]>=threshold:
            to_drop.add(corr_matrix.columns[j])

print(to_drop)

X_reduced=X.drop(columns=to_drop)
X_reduced.shape

import sklearn
from sklearn.feature_selection import mutual_info_regression

# Dictionary to hold MI scores for each target month
mi_results = {}

# Loop over each target month and compute MI
for month in y.columns:
    mi = mutual_info_regression(X_reduced, y[month], random_state=42)
    mi_results[month] = pd.Series(mi, index=X_reduced.columns)

# Combine into a DataFrame
mi_df = pd.DataFrame(mi_results)

# Add an average MI column across all months
mi_df["Avg_MI"] = mi_df.mean(axis=1)

# Sort by average mutual information
mi_df_sorted = mi_df.sort_values(by="Avg_MI", ascending=False)

# Display results
pd.set_option("display.max_rows", None)  # Optional: see all features
print(mi_df_sorted)

top_features = mi_df_sorted.head(16).index.tolist()
X_top_16 = X_reduced[top_features]
X_top_16.columns

X_top_16.shape

X_top_16.head()

!pip install scikit-learn==1.5.2

!pip install scikit-learn==1.2.2

!pip uninstall -y scikit-learn
!pip install scikit-learn==1.5.2

pip install --upgrade scikit-learn scikeras

# Required installations (only needed in Colab/initial setup)
!pip install scikeras tensorflow keras xgboost

# --- Step 1: Imports ---
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold, GridSearchCV, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense, Conv1D, Flatten, Input, Dropout
from scikeras.wrappers import KerasRegressor
import tensorflow as tf
import xgboost as xgb

# --- Step 2: Data Loading ---
#data = pd.read_csv('/content/rearranged_bms_data.csv')  # Load your actual data

# --- Step 3: Define Features and Targets ---
top_16_features = [
    'Plasticity Index', 'Liquid Limit', 'Angle of internal friction (Degress)',
    'Cohesion (C, kPa) ', 'Fines content (%)', 'Sand (%)', 'Density (kN/m3)',
    'Silt (%)', 'Natural water content (%)', 'Gravel (%)', 'Specific Gravity',
    'Plants of location', 'Green Color of soil', 'White Color of soil',
    'Earthy smell of soil', 'Water retention of soil'
]

targets = ['11-24', '3-25', '5-25']

target_to_dynamic_months = {
    '11-24': ['9-23', '10-23', '11-23', '9-24', '10-24'],
    '3-25' : ['10-23', '11-23', '9-24', '10-24', '11-24'],
    '5-25' : ['10-23', '11-23', '9-24', '10-24', '11-24']
}

# --- NEW Step 3.5: Train-Test Split ---

train_test_data = {}
for target in targets:
    features = top_16_features + target_to_dynamic_months[target]
    X = data[features]
    y = data[target]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    train_test_data[target] = (X_train, X_test, y_train, y_test)
#----early stopping---

from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# --- Step 4: Deep Model Definitions ---
def create_lstm_model(input_shape):
    model = Sequential()
    model.add(Input(shape=input_shape))
    model.add(LSTM(50, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

def create_gru_model(input_shape):
    model = Sequential()
    model.add(Input(shape=input_shape))
    model.add(GRU(50, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

def create_cnn_model(input_shape):
    model = Sequential()
    model.add(Input(shape=input_shape))
    model.add(Conv1D(64, 2, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Flatten())
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

# --- Step 4.5: RMSEPE Helper Function ---
def rmsepe(y_true, y_pred, min_denom=0.05):
    """
    RMSEPE with denominator clipping for stability on small target values.
    """
    y_true = np.array(y_true)
    denom = np.maximum(np.abs(y_true), min_denom)  # prevent division by tiny y_true
    return np.sqrt(np.mean(np.square((y_pred - y_true) / denom))) * 100


# --- Step 5: Meta-model Setup (moved up here to avoid NameError) ---
def train_stacking_model(oof_preds, y, meta_model, meta_param_grid):
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    meta_grid_search = GridSearchCV(meta_model, meta_param_grid, cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)
    meta_grid_search.fit(oof_preds, y)
    best_meta_model = meta_grid_search.best_estimator_
    y_pred = best_meta_model.predict(oof_preds)
    rmse = np.sqrt(mean_squared_error(y, y_pred))
    r2 = r2_score(y, y_pred)
    rmsepe_val = rmsepe(y, y_pred)
    print(f'Stacking Model: {meta_model}')
    print(f'Best Parameters (Meta-Model): {meta_grid_search.best_params_}')
    print(f'RMSE: {rmse}')
    print(f'R^2: {r2}')
    print(f'RMSEPE: {rmsepe_val:.2f}%')
    print('-'*50)

    return best_meta_model

# --- Step 6: Base Model Training ---
def train_base_models(X, y, models):
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    oof_preds = np.zeros((X.shape[0], len(models)))

    for i, (model_name, (model_func, param_grid)) in enumerate(models.items()):
        if model_name in ['LSTM', 'GRU', 'CNN']:
            X_scaled_reshaped = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))
            kf_splits = kf.split(X_scaled_reshaped)
        else:
            kf_splits = kf.split(X_scaled)

        for train_index, val_index in kf_splits:
            X_train_fold, X_val_fold = X_scaled[train_index], X_scaled[val_index]
            y_train_fold, y_val_fold = y[train_index], y[val_index]

            if model_name in ['LSTM', 'GRU', 'CNN']:
                X_train_fold = X_train_fold.reshape((X_train_fold.shape[0], X_train_fold.shape[1], 1))
                X_val_fold = X_val_fold.reshape((X_val_fold.shape[0], X_val_fold.shape[1], 1))
                model = KerasRegressor(model=model_func, input_shape=(X_train_fold.shape[1], 1))
                model.fit(X_train_fold, y_train_fold,
                          epochs=100,
                          batch_size=32,
                          verbose=0,
                          validation_split=0.2,
                          callbacks=[early_stopping])

                preds = model.predict(X_val_fold).flatten()
            else:
                grid_search = GridSearchCV(model_func(), param_grid, cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)
                grid_search.fit(X_train_fold, y_train_fold)
                best_model = grid_search.best_estimator_
                preds = best_model.predict(X_val_fold)

            oof_preds[val_index, i] = preds

        oof_rmse = np.sqrt(mean_squared_error(y, oof_preds[:, i]))
        oof_r2 = r2_score(y, oof_preds[:, i])
        print(f'Model: {model_name}')
        print(f'Best Parameters: {grid_search.best_params_}' if model_name not in ['LSTM', 'GRU', 'CNN'] else "")
        print(f'OOF RMSE: {oof_rmse}')
        print(f'OOF R^2: {oof_r2}')
        print('-'*50)

    return oof_preds

# --- Step 7: Define Models ---
models = {
    'LinearRegression': (LinearRegression, {}),
    'DecisionTree': (DecisionTreeRegressor, {'max_depth': [None, 10, 20], 'min_samples_split': [2, 10, 20], 'min_samples_leaf': [1, 5, 10]}),
    'RandomForest': (RandomForestRegressor, {'n_estimators': [100, 200], 'max_depth': [None, 10], 'min_samples_split': [2, 10]}),
    'SVM': (SVR, {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'kernel': ['linear', 'rbf']}),
    'kNN': (KNeighborsRegressor, {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance'], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}),
    'NeuralNetwork': (MLPRegressor, {'hidden_layer_sizes': [(100,), (50, 50)], 'activation': ['relu', 'tanh'], 'alpha': [0.0001, 0.001]}),
    'XGBoost': (xgb.XGBRegressor, {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.1], 'max_depth': [3, 6]}),
    'LSTM': (create_lstm_model, {}),
    'GRU': (create_gru_model, {}),
    'CNN': (create_cnn_model, {})
}



# --- Step 8: Define Meta-models ---
meta_model_lr = LinearRegression()
meta_param_grid_lr = {'fit_intercept': [True, False], 'positive': [True, False]}

meta_model_lasso = Lasso()
meta_param_grid_lasso = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1], 'max_iter': [1000, 2000, 5000]}

meta_model_xgb = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
meta_param_grid_xgb = {'n_estimators': [50, 100, 250], 'learning_rate': [0.01, 0.1], 'max_depth': [3, 6]}

# --- Step 9: Model Training Loop ---
oof_predictions_dict = {}
stacking_models_dict = {}

for target in targets:
    print(f"\n\n==============================")
    print(f"Training for Target: {target}")
    print(f"==============================")
    dynamic_months = target_to_dynamic_months[target]
    features = top_16_features + dynamic_months
    X = data[features]
    y = data[target]
    oof_preds = train_base_models(X, y, models)
    oof_predictions_dict[target] = oof_preds

    print(f"\nStacking with Linear Regression for {target}:")
    stack_lr = train_stacking_model(oof_preds, y, meta_model_lr, meta_param_grid_lr)
    print(f"\nStacking with Lasso Regression for {target}:")
    stack_lasso = train_stacking_model(oof_preds, y, meta_model_lasso, meta_param_grid_lasso)
    print(f"\nStacking with XGBoost for {target}:")
    stack_xgb = train_stacking_model(oof_preds, y, meta_model_xgb, meta_param_grid_xgb)

    stacking_models_dict[target] = {
        "LinearRegression": stack_lr,
        "Lasso": stack_lasso,
        "XGBoost": stack_xgb
    }

# --- Step 9.5: Evaluate on Test Set ---
from sklearn.metrics import mean_squared_error, r2_score

for target in targets:
    print(f"\n--- Test Set Evaluation for {target} ---")
    X_train, X_test, y_train, y_test = train_test_data[target]
    dynamic_months = target_to_dynamic_months[target]
    features = top_16_features + dynamic_months

    scaler = StandardScaler()
    scaler.fit(X_train)
    X_train_scaled = scaler.transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    base_test_preds = []
    print(f"\n Test Performance of Base Models for {target}")
    for i, (model_name, (model_func, param_grid)) in enumerate(models.items()):
        print(f"\n→ Base Model: {model_name}")
        if model_name in ['LSTM', 'GRU', 'CNN']:
            X_train_r = X_train_scaled.reshape(-1, len(features), 1)
            X_test_r = X_test_scaled.reshape(-1, len(features), 1)
            model = KerasRegressor(model=model_func, input_shape=(len(features), 1))
            model.fit(X_train_r, y_train,
                      epochs=100,
                      batch_size=32,
                      verbose=0,
                      validation_split=0.2,
                      callbacks=[early_stopping])

            pred = model.predict(X_test_r).flatten()
        else:
            model_cv = GridSearchCV(model_func(), param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
            model_cv.fit(X_train, y_train)
            best_model = model_cv.best_estimator_
            pred = best_model.predict(X_test)

        base_test_preds.append(pred)

        # Print test metrics
        rmse = np.sqrt(mean_squared_error(y_test, pred))
        r2 = r2_score(y_test, pred)
        rmsepe_val = rmsepe(y_test, pred)
        print(f"  RMSE   : {rmse:.4f}")
        print(f"  R²     : {r2:.4f}")
        print(f"  RMSEPE : {rmsepe_val:.2f}%")

    base_test_preds = np.stack(base_test_preds, axis=1)

    # Meta-model test predictions
    print(f"\n Test Performance of Stacked Models for {target}")
    for stacker_name, stacker_model in stacking_models_dict[target].items():
        stack_test_pred = stacker_model.predict(base_test_preds)
        rmse = np.sqrt(mean_squared_error(y_test, stack_test_pred))
        r2 = r2_score(y_test, stack_test_pred)
        rmsepe_val = rmsepe(y_test, stack_test_pred)

        print(f"\nStacked Model: {stacker_name}")
        print(f"  RMSE   : {rmse:.4f}")
        print(f"  R²     : {r2:.4f}")
        print(f"  RMSEPE : {rmsepe_val:.2f}%")
        print("-" * 40)

# --- Step 10 (UPDATED): Save Train & Test Predictions ---
for target in targets:
    oof_preds = oof_predictions_dict[target]
    stackers = stacking_models_dict[target]

    # Save training predictions (from OOF)
    data[f'Pred_{target}_LR'] = stackers['LinearRegression'].predict(oof_preds)
    data[f'Pred_{target}_Lasso'] = stackers['Lasso'].predict(oof_preds)
    data[f'Pred_{target}_XGBoost'] = stackers['XGBoost'].predict(oof_preds)

    # Save test predictions
    X_train, X_test, y_train, y_test = train_test_data[target]
    scaler = StandardScaler()
    scaler.fit(X_train)
    X_test_scaled = scaler.transform(X_test)

    base_test_preds = []
    for model_name, (model_func, param_grid) in models.items():
        if model_name in ['LSTM', 'GRU', 'CNN']:
            X_train_scaled = scaler.transform(X_train).reshape(-1, len(X_train.columns), 1)
            X_test_scaled_r = X_test_scaled.reshape(-1, len(X_train.columns), 1)
            model = KerasRegressor(model=model_func, input_shape=(len(X_train.columns), 1))
            model.fit(X_train_scaled, y_train,
                      epochs=100,
                      batch_size=32,
                      verbose=0,
                      validation_split=0.2,
                      callbacks=[early_stopping])

            pred = model.predict(X_test_scaled_r).flatten()
        else:
            model_cv = GridSearchCV(model_func(), param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
            model_cv.fit(X_train, y_train)
            best_model = model_cv.best_estimator_
            pred = best_model.predict(X_test)
        base_test_preds.append(pred)

    base_test_preds = np.stack(base_test_preds, axis=1)

    for stacker_name, stacker_model in stackers.items():
        test_pred = stacker_model.predict(base_test_preds)
        data.loc[X_test.index, f'TestPred_{target}_{stacker_name}'] = test_pred

# --- Step 11: Save Everything to CSV ---
data.to_csv('ML_Data_18_predictions.csv', index=False)
print("ML_Data_18_predictions.csv with train & test predictions saved successfully.")

!pip install shap

# Uninstall current versions
!pip uninstall -y scikit-learn xgboost shap

# Install compatible versions
!pip install scikit-learn==1.3.2 xgboost==1.7.6 shap==0.41.0

# STEP 1: Imports and Data Loading
import shap
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

shap.initjs()

# Load your landslide dataset
data = pd.read_csv('ML_Data_18 - ML_Data.csv (1).csv')

# Step 1: Define features and targets
features = [
    'Plasticity Index', 'Liquid Limit', 'Angle of internal friction (Degress)',
    'Cohesion (C, kPa) ', 'Fines content (%)', 'Sand (%)', 'Density (kN/m3)',
    'Silt (%)', 'Natural water content (%)', 'Gravel (%)', 'Specific Gravity',
    'Vegetation Cover', 'Green Color of soil', 'White Color of soil',
    'Earthy smell of soil', 'Water retention of soil'
]

target_1m = '11-24'
target_3m = '3-25'
target_6m = '5-25'

# Step 2: Prepare the shared feature matrix
X = data[features]
y_1m = data[target_1m]
y_3m = data[target_3m]
y_6m = data[target_6m]

# Step 3: Standardize features once (shared across targets)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# --- Train XGBoost for 1-Month Velocity Prediction (11-24) ---
xgb_1m = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3,
                          objective='reg:squarederror', random_state=42)
xgb_1m.fit(X_scaled, y_1m)

# --- Train XGBoost for 3-Month Velocity Prediction (3-25) ---
xgb_3m = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3,
                          objective='reg:squarederror', random_state=42)
xgb_3m.fit(X_scaled, y_3m)

# --- Train XGBoost for 6-Month Velocity Prediction (5-25) ---
xgb_6m = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3,
                          objective='reg:squarederror', random_state=42)
xgb_6m.fit(X_scaled, y_6m)

import numpy as np
np.int = int  # Monkey patch deprecated alias

# === Use TreeExplainer for XGBoost — 1-Month Velocity (11-24) ===
explainer_1m = shap.TreeExplainer(xgb_1m, feature_perturbation="tree_path_dependent")
shap_values_1m = explainer_1m.shap_values(X_scaled)

# === Use TreeExplainer for XGBoost — 3-Month Velocity (3-25) ===
explainer_3m = shap.TreeExplainer(xgb_3m, feature_perturbation="tree_path_dependent")
shap_values_3m = explainer_3m.shap_values(X_scaled)

# === Use TreeExplainer for XGBoost — 6-Month Velocity (5-25) ===
explainer_6m = shap.TreeExplainer(xgb_6m, feature_perturbation="tree_path_dependent")
shap_values_6m = explainer_6m.shap_values(X_scaled)

# === SHAP Summary Plot — 1-Month Velocity (11-24) ===
plt.figure(figsize=(10, 6))
shap.summary_plot(shap_values_1m, features=X, feature_names=features)

# === SHAP Summary Plot — 6-Month Velocity (5-25) ===
plt.figure(figsize=(10, 6))
shap.summary_plot(shap_values_6m, features=X, feature_names=features)

